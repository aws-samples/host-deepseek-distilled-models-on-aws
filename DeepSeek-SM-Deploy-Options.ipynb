{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe102c4-efe4-43eb-ab39-27cfc65691a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy DeepSeek Distilled Models on SageMaker\n",
    "\n",
    "This notebook walks you through various strategies of deploying [distilled models from DeepSeek R1](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d). This includes:\n",
    "    \n",
    "- DeepSeek-R1-Distill-Llama-70B using Hugginface Text Generation Inference (TGI) on SageMaker Endpoint\n",
    "- DeepSeek-R1-Distill-Llama-70B using Deep Java Library and vLLM backend on SageMaker Endpoint\n",
    "- DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Qwen-7B on a single SageMaker Endpoint using Inference Components\n",
    "\n",
    "However, this implementation applies to any of the distilled deepseek models on HuggingFace in the aforementioned link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fe891-0d63-4d9e-a52b-4734c60d7cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sagemaker -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72784abf-47ca-420a-a7bd-a9034076e559",
   "metadata": {},
   "source": [
    "## Using HuggingFace TGI for Serving DeepSeek-R1-Distill-Llama-70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d792a-ea17-423a-9d11-5c22f93e8658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
    "\t'SM_NUM_GPUS': json.dumps(8) # Change this based on the GPU used, ml.g6.48xlarge has 8 GPUs\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"2.3.1\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g6e.48xlarge\", # has 8 GPUs with a total GPU memeory of 384 GB\n",
    "\tcontainer_startup_health_check_timeout=3600,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9bfdd-ec4d-4cd4-bedf-cd715749b486",
   "metadata": {},
   "source": [
    "### Invoke the SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "44d90af3-bee5-4f34-810d-033807ae3688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streamin helper class\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import re\n",
    "\n",
    "NEWLINE = re.compile(r'\\\\n')  \n",
    "DOUBLE_NEWLINE = re.compile(r'\\\\n\\\\n')\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream from Llama 2 model inferenced with LMI Container. \n",
    "    \n",
    "    The output of the model will be in the following repetetive but incremental format:\n",
    "    ```\n",
    "    b'{\"generated_text\": \"'\n",
    "    b'lo from L\"'\n",
    "    b'LM \\\\n\\\\n'\n",
    "    b'How are you?\"}'\n",
    "    ...\n",
    "\n",
    "    For each iteration, we just read the incremental part and seek for the new position for the next iteration till the end of the line.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream, cc_schema):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        self.cc_schema = cc_schema\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        start_sequence = b'{\"generated_text\": \"'\n",
    "        stop_sequence = b'\"}'\n",
    "        new_line = '\\n'\n",
    "        double_new_line = '\\n\\n'\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line:\n",
    "                self.read_pos += len(line)\n",
    "                if line.startswith(start_sequence):# in :\n",
    "                    line = line.lstrip(start_sequence)\n",
    "                \n",
    "                if line.endswith(stop_sequence):\n",
    "                    line =line.rstrip(stop_sequence)\n",
    "                line = line.decode('utf-8')\n",
    "                line = NEWLINE.sub(new_line, line)\n",
    "                line = DOUBLE_NEWLINE.sub(double_new_line, line) \n",
    "                if self.cc_schema:\n",
    "                    pattern = r'\"content\"\\s*:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"'  # snipping out the \"content :\" key that holds the model response as its value (For DJL)\n",
    "                else: \n",
    "                    pattern = r'\"text\"\\s*:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"'  # snipping out the \"text :\" key that holds the model response as its value (For TGI)\n",
    "                match = re.search(pattern, line)\n",
    "                # print(line)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "                else:\n",
    "                    return \"\"\n",
    "                    \n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# SageMaker Runtime API for Inference with Streaming\n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    return response_stream\n",
    "\n",
    "\n",
    "def print_response_stream(response_stream,  cc_schema = False):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    for line in LineIterator(event_stream, cc_schema):  \n",
    "        print(line.strip(), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5c42075c-e0a1-4d12-90f4-e1b8f18aa19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt template for the distilled llama model\n",
    "\n",
    "def prompt_template(system_message, query, stream):\n",
    "\n",
    "    # DeepSeek does not recommend using system prompts and recommends adding it to the user prompt\n",
    "    user_question = f\"{system_message}\\n\\n{query}\"\n",
    "    payload={'inputs':  f\"\"\"<｜begin▁of▁sentence｜><｜User｜>{user_question}<｜Assistant｜>\"\"\", \n",
    "     'parameters': {'max_new_tokens': 1050, 'top_p': 0.9, 'temperature': 0.1, \"return_full_text\": False, \"stop\" : [\"<｜end▁of▁sentence｜>\"]}, \"stream\": stream}\n",
    "\n",
    "    # template with system message parameter\n",
    "    # payload={'inputs':  f\"\"\"<｜begin▁of▁sentence｜>{system_message}<｜User｜>{query}<｜Assistant｜>\"\"\", \n",
    "    #  'parameters': {'max_new_tokens': 500, 'top_p': 0.9, 'temperature': 0.1, \"return_full_text\": False}}\n",
    "    \n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "56ebc54b-3528-448d-9f61-376b85ba31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>    Okay  ,  so  I  need  to  figure  out  what  the  most  expensive  gem  is  .  I  'm  not  really  a  gem  expert  ,  but  I  know  a  few  things  .  Let  me  start  by  thinking   about  what  makes  a  gem  expensive  .  It   's  probably  a  combination  of  rarity  ,  beauty  ,  and  maybe  historical  significance  .    I  remember   hearing  about  diamonds  being  really  valuable  ,  especially  large  ,  flawless   ones  .  The  \\\"  Pink  Star  \\\"  comes  to  mind  ;  I  think  it  's  a  pink  diamond  that  sold  for  a  lot  .  But  I  'm  not  sure  if  it  's  the  most  expensive  .  Then  there  's  the  \\\"  C  ull  inan  Diamond  ,\\\"  which  I  think  is  also  very  valuable  ,  but  I  'm  not  certain   about  its  price  .  I  also  recall  that  some  colored  diamonds  ,  like  red  diamonds   ,  are  extremely  rare  .  The  \\\"  M  ou  ssa  ie  ff  Red  Diamond  \\\"  might  be  one  of  them  .  It  's  supposed  to  be  the  largest   known  red  diamond  ,  so  that  could  make  it  very  expensive  .    J  ade  ite  is  another   gem  stone  I  've  heard  is  highly  prized  ,  especially   in  Asian  markets  .  The  \\\"  H  utton  -M  div  ani  Jade  ite  Necklace  \\\"  was  auction   ed  off  for  a  lot  ,  but  I  'm  not  sure  how  it  compares  to  the  top  diamonds   .    Then  there  's  the  \\\"  Ben  ito  ite  ,\\\"  which  is  a  rare  gem  from  San  Ben  ito  County  in  California  .  It  's  the  official   state  gem  stone  ,  but  I  don  't  know  its  value  relative  to  others  .    I  should  also  consider  other  factors  like  market  demand  and  historical  value  .  Some  gems  might  be  more  expensive  because  they  're  part  of  famous  jewelry  or  have  a  rich   history  .  For  example  ,  the  \\\"  Or  lov  Diamond  \\\"  has  a  legendary  story  ,  but  I  'm  not  sure  about  its  current  value  .    I  think  the  key  here  is  to  look  at  recent  auction  records  because  that  's  where  the  most  expensive  gems  are  usually  sold  .  The  \\\"  Pink  Star  \\\"  was  sold  at  auction  for  over  $  70  million  ,  which  seems  really  high  .  The  \\\"  C  ull  inan  \\\"  might  have  a  higher  value  ,  but  I  'm   not  sure  if  it  's  been  sold  recently  or  if  it  's  even  for  sale  .    So  ,  putting   it  all  together  ,  I  believe  the  most  expensive  gem  is  likely   the  \\\"  Pink  Star  \\\"  diamond  ,  but  I  should   double  -check  to  make  sure  there  isn  't  a  more  recent  or  higher  -priced  gem  that  I  'm  not  aware  of  .  Also  ,  considering  the  car  at  weight  and  quality  ,  larger  ,  flawless  stones  are  going  to  be  at  the  top  of  the  list  .  </think>    The  most  expensive  gem  stone  ,  based  on  recent  auction  records  and  considering  factors  like  rarity  ,  quality  ,  and  market   demand  ,  is  the  \\\"  Pink  Star  \\\"  diamond   .  It  is  a    59  .  60  -car  at  oval  -cut  pink  diamond  that  sold  for  $  71  .  2  million   at  an  auction  in  Hong  Kong  .  This  sale  highlights  its  exceptional   value  due  to  its  size  ,  color  ,  and  flawless   quality  .  While  other  notable  gems  like  the  \\\"  C  ull  inan  Diamond  \\\"  and  \\\"  H  utton  -M  div  ani  Jade  ite  Necklace  \\\"  are  also  highly  valuable  ,  the  \\\"  Pink  Star  \\\"  currently  holds  the  top  position   in  terms  of  auction  price  .  <｜end▁of▁sentence｜>        "
     ]
    }
   ],
   "source": [
    "# send a request to the sagemaker endpoint\n",
    "system_message=\"\"\"\n",
    "You are a Chatty Assitant\n",
    "\"\"\"\n",
    "query= \"What is the most expensive gem?\"\n",
    "\n",
    "stream = True #to stream response or not\n",
    "\n",
    "payload = prompt_template(system_message, query, stream)\n",
    "\n",
    "if not stream:\n",
    "    response = predictor.predict(\n",
    "    payload\n",
    "    )\n",
    "\n",
    "    print(response[0]['generated_text'])\n",
    "else:\n",
    "    smr_client = boto3.client('sagemaker-runtime')\n",
    "    response_stream = get_realtime_response_stream(smr_client, predictor.endpoint_name, payload)\n",
    "    print_response_stream(response_stream)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201210f3-a085-4db9-a276-f884612d65a6",
   "metadata": {},
   "source": [
    "### To invoke an existing SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558e0dc-ac65-4375-8771-ba0628d4c17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "predictor1 = Predictor(endpoint_name=\"ENDPOINT NAME\")\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a Chatty Assitant\n",
    "\"\"\"\n",
    "query= \"What is the most expensive gem?\"\n",
    "\n",
    "user_question = f\"{system_message}\\n\\n{query}\"\n",
    "payload={'inputs':  f\"\"\"<｜begin▁of▁sentence｜><｜User｜>{user_question}<｜Assistant｜>\"\"\", \n",
    " 'parameters': {'max_new_tokens': 500, 'top_p': 0.9, 'temperature': 0.1, \"return_full_text\": False}}\n",
    "\n",
    "response = predictor1.predict(json.dumps(payload),\n",
    "                  initial_args={\"ContentType\": \"application/json\"}\n",
    "                 )\n",
    "\n",
    "print(json.loads(response)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1c5ab-ad9f-49c8-98ab-87ebe0942bbe",
   "metadata": {},
   "source": [
    "# Using DJL with vLLM for serving DeepSeek-R1-Distill-Llama-70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "898ef517-e547-4a0d-8f7e-9e02ddfade09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c7c2701b-1390-4579-9ee9-307b828b9528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/29/25 19:39:10] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">528</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/29/25 19:39:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=24049;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=862356;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\u001b\\\u001b[2m528\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B' # model will be download form Huggingface hub\n",
    "\n",
    "env = {\n",
    "    \"TENSOR_PARALLEL_DEGREE\": \"8\",            # use 8 GPUs, modify baed on instance types\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",           # use vllm for rolling batching\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "   }\n",
    "role = sagemaker.get_execution_role()\n",
    "model = DJLModel(\n",
    "    model_id=model_id,\n",
    "    env=env,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9b231-8a47-4de4-8e9b-f7ecc9c99777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g6e.48xlarge\" # has 8 GPUs with a total GPU memeory of 384 GB\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             # endpoint_name=\"djl-llama-70-distil-r1\",\n",
    "             container_startup_health_check_timeout=3600\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5d32bcd6-7df4-46e8-97cf-8861503cdf12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well , let me think . There 's Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uran us , and Neptune . Wait , didn 't Pluto used to be considered a planet ? Yeah , I remember  hearing that it was re classified as a dwarf planet a while back . So , now we have eight planets in our solar system .  Each of these  planets has unique features . Mercury is the closest  to the Sun and is really hot during the day  but freezing at night . Venus is known for being the hottest planet , even hotter than Mercury , because  of its thick atmosphere that traps heat . Earth  is where we live , and it 's    CPU times: user 40 ms, sys: 4.56 ms, total: 44.5 ms\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Use Inference API Schema\n",
    "stream = True\n",
    "payload = {\"inputs\": \"What are the planets in our solar system?\", \"parameters\": {\"max_new_tokens\":128,'temperature': 0.6,}, \"stream\":stream}\n",
    "\n",
    "if not stream:\n",
    "    response = predictor.predict(\n",
    "    payload\n",
    "    )\n",
    "\n",
    "    print(response['generated_text'])\n",
    "else:\n",
    "    smr_client = boto3.client('sagemaker-runtime')\n",
    "    response_stream = get_realtime_response_stream(smr_client, predictor.endpoint_name, payload)\n",
    "    print_response_stream(response_stream)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "dddc01ae-7503-40dd-83c3-d2fcb14a3f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'message': {'role': 'assistant', 'content': '<think>\\nOkay, so I need to understand what deep learning is. I\\'ve heard the term before, especially in the context of AI and machine learning, but I\\'m not entirely sure what it entails. Let me start by breaking down the term: \"deep learning.\" It must be a subset of machine learning, which itself is part of artificial intelligence. \\n\\nI remember that machine learning involves training models to make predictions or decisions based on data. Traditional machine learning requires a lot of feature engineering, where humans manually select which features of the data are important. But deep learning, I think, is different because it can learn these features automatically. That must be why it\\'s called \"deep\"—because it uses multiple layers to learn hierarchical representations of data.\\n\\nSo, deep learning uses neural networks, inspired by the human brain. Each layer in these networks processes the data at different levels. For example, in image recognition, the first layer might detect edges, the next might detect shapes, and so on until the final layer recognizes the object. This hierarchical learning means the model can capture complex patterns without manual feature engineering.\\n\\nI\\'m a bit fuzzy on the types of neural networks used in deep learning. I know about Convolutional Neural Networks (CNNs), which I think are used for'}, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "CPU times: user 14.2 ms, sys: 0 ns, total: 14.2 ms\n",
      "Wall time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Use Chat Completion Schema\n",
    "stream = False\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is deep learning?\"\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\":256,\n",
    "    \"temperature\": 0.6,\n",
    "    \"stream\": stream\n",
    "  }\n",
    "## Use Chat Completions API Schema\n",
    "\n",
    "if not stream:\n",
    "    response = predictor.predict(\n",
    "    payload\n",
    "    )\n",
    "\n",
    "    print(response['choices'])\n",
    "else:\n",
    "    smr_client = boto3.client('sagemaker-runtime')\n",
    "    response_stream = get_realtime_response_stream(smr_client, predictor.endpoint_name, payload)\n",
    "    print_response_stream(response_stream, cc_schema=True)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617e1f2-f3ec-470b-997d-9d5bf43e6bc2",
   "metadata": {},
   "source": [
    "## Using SageMaker Inference Componenets to serve R1- Distilled Llamma 8b and Qwen 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "47c1fff7-ca16-472c-81d6-3feda04c5056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaeac3a-9903-4610-ba26-b1207eba615d",
   "metadata": {},
   "source": [
    "#### Create the endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "318de17b-9861-4737-bc97-58f477b881f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:715253196401:endpoint/r1-distilled-ic-model-v2',\n",
       " 'ResponseMetadata': {'RequestId': 'eb8fcf39-661f-4222-85cc-52546cf95875',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'eb8fcf39-661f-4222-85cc-52546cf95875',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '92',\n",
       "   'date': 'Wed, 29 Jan 2025 20:11:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Endpoint Configuration for serving the models\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "endpoint_config_name = \"r1-distilled-model-v2\"\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[{\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "        \"InitialInstanceCount\": 1,\n",
    "\t\t\"RoutingConfig\": {\n",
    "            \"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"\n",
    "        }\n",
    "    }]\n",
    ")\n",
    "\n",
    "endpoint_name = \"r1-distilled-ic-model-v2\"\n",
    "sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7722e-c5e1-4237-898b-8fe893a5cc6b",
   "metadata": {},
   "source": [
    "#### Create the SageMaker Model objects for teh models of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ea7056fe-d857-4928-8253-b99b8e95b0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/29/25 20:11:35] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">528</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/29/25 20:11:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=858739;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=533652;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\u001b\\\u001b[2m528\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: r1-llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8b-distill                        <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: r1-llama-\u001b[1;36m3\u001b[0m-8b-distill                        \u001b]8;id=102672;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=497831;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a SageMaker model object for llama 70b with DJL and LMI-dist serving properties\n",
    "\n",
    "model_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B' # model will be download form Huggingface hub\n",
    "\n",
    "env = {\n",
    "    \"TENSOR_PARALLEL_DEGREE\": \"2\",            # use 2 GPUs out of, modify baed on instance types\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",           # use LMI-Dist for rolling batching\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "   }\n",
    "role = sagemaker.get_execution_role()\n",
    "llama_model = \"r1-llama-3-8b-distill\"\n",
    "model = DJLModel(\n",
    "    model_id=model_id,\n",
    "    env=env,\n",
    "    name = llama_model,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    role=role)\n",
    "\n",
    "model.create(instance_type=\"ml.g5.12xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "38d1bed9-9dde-422c-8e09-d002cd4ef96d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/29/25 20:11:37] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">528</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/29/25 20:11:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=288031;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=360242;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#528\u001b\\\u001b[2m528\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: r1-qwen-7b-distill                           <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: r1-qwen-7b-distill                           \u001b]8;id=379156;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=903057;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a SageMaker model object for Qwen 32B with DJL and vLLM serving properties\n",
    "\n",
    "model_id = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B' # model will be download form Huggingface hub\n",
    "\n",
    "env = {\n",
    "    \"TENSOR_PARALLEL_DEGREE\": \"2\",            # use 2 GPUs, modify baed on instance types\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",           # use vllm for rolling batching\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "   }\n",
    "role = sagemaker.get_execution_role()\n",
    "qwen_model = \"r1-qwen-7b-distill\"\n",
    "model = DJLModel(\n",
    "    model_id=model_id,\n",
    "    env=env,\n",
    "    name = qwen_model,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    role=role)\n",
    "\n",
    "model.create(instance_type=\"ml.g5.12xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498ff09-e878-4836-a1f0-b8c16c62dcb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check the status of the endpoint until it has successfully deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1fe0b046-d188-42b7-8911-6b166514e8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: InService\n",
      "Endpoint 'r1-distilled-ic-model-v2' is now in service!\n",
      "Endpoint status check completed.\n"
     ]
    }
   ],
   "source": [
    "# Check Status of endpoint\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response['EndpointStatus']\n",
    "        print(f\"Current status: {status}\")\n",
    "\n",
    "        if status == 'InService':\n",
    "            print(f\"Endpoint '{endpoint_name}' is now in service!\")\n",
    "            break\n",
    "        elif status in ['Creating', 'Updating']:\n",
    "            print(f\"Endpoint '{endpoint_name}' is still {status.lower()}. Waiting...\")\n",
    "            time.sleep(30)  # Sleep for 30 seconds before checking again\n",
    "        else:\n",
    "            print(f\"Endpoint status is '{status}'. This may indicate an issue.\")\n",
    "            break\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            print(f\"Endpoint '{endpoint_name}' not found.\")\n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"Endpoint status check completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98d33e-e499-41b4-9e8c-3a56bc459ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Inference Componenets of the various models to be hosted behind the endpoint with their respective resource configs\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=qwen_model,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=\"AllTraffic\",\n",
    "    Specification={\n",
    "        \"ModelName\": qwen_model,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "\t\t    \"NumberOfAcceleratorDevicesRequired\": 2, # Number of GPUs\n",
    "\t\t\t\"NumberOfCpuCoresRequired\": 20, # Number of CPU cores  (total vCPU // (number of replica * number if inference componenets) - more for management) Based on 48vCPU for the selected instance\n",
    "\t\t\t\"MinMemoryRequiredInMb\": 20*1024 # Minimum memory in MB (total CPU memory // (number of replica * number if inference componenets) - more for management) Based on 192 GB of the selected instance\n",
    "\t    }\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1}, # Number of replicas\n",
    ")\n",
    "\n",
    "# Inference component for FLAN-T5 XXL\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=llama_model,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=\"AllTraffic\",\n",
    "    Specification={\n",
    "        \"ModelName\": llama_model,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "\t\t    \"NumberOfAcceleratorDevicesRequired\": 2, \n",
    "\t\t\t\"NumberOfCpuCoresRequired\": 20, \n",
    "\t\t\t\"MinMemoryRequiredInMb\": 20*1024\n",
    "\t    }\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "08206619-18ce-40d7-bd2b-5f79818e5e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \"<think>\\nOkay, so I need to figure out why California is a great place to live. I'm not super familiar with California, but I know it's a big state in the U.S. I'll start by thinking about what makes a place good to live in. Probably things like climate, job opportunities, cost of living, culture, and maybe natural beauty. \\n\\nFirst, the climate. I've heard that California has a mild climate, which is nice because not everyone can handle extreme cold\"}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the endpoints\n",
    "def invoke_sagemaker_endpoint_with_retry(endpoint_name, inference_component_name, payload, max_retries=5, initial_delay=10):\n",
    "\n",
    "    sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                InferenceComponentName = inference_component_name,\n",
    "                ContentType=\"application/json\",\n",
    "                Accept=\"application/json\",\n",
    "                Body=json.dumps(payload),\n",
    "            )\n",
    "\n",
    "            result = response['Body'].read().decode()\n",
    "            print(result)\n",
    "            return result\n",
    "\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ValidationError' and 'Inference Component has no capacity' in str(e):\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_delay * (2 ** attempt)  # Exponential backoff\n",
    "                    print(f\"Attempt {attempt + 1} failed. SageMaker still deploying Inference Components, Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Max retries reached. Last error: {e}\")\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"All retry attempts failed.\")\n",
    "    return None\n",
    "\n",
    "system_message = \"AI Assitant\"\n",
    "query = \"Why is California a great place to live?\"\n",
    "\n",
    "payload={'inputs':  f\"\"\"<｜begin▁of▁sentence｜>{system_message}<｜User｜>{query}<｜Assistant｜>\"\"\", \n",
    "     'parameters': {'max_new_tokens': 100, 'top_p': 0.9, 'temperature': 0.6, \"return_full_text\": False}}   \n",
    "\n",
    "inference_c_name = qwen_model #llama_model\n",
    "result = invoke_sagemaker_endpoint_with_retry(endpoint_name, inference_c_name, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ba7be-69c2-4865-a012-1016dee5a682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
